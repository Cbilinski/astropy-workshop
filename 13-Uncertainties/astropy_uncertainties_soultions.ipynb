{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An overview of `astropy.uncertainty`:\n",
    "\n",
    "A relatively recent addition to `astropy` is the `astropy.uncertainty` sub-package.  Its primary purpose is to represent the *uncertainties* of Quantities in a way that allows relatively straightforward application of error-propogation rules.\n",
    "\n",
    "Some important caveats for `astropy.uncertainty`: it is *not* intended to be a fully-featured replacement for thorough statistical analysis. For that you will want to use more statistical modeling approaches like combining statistically-oriented fitting tools with `astropy` pieces for just the astro-specific parts. E.g., you might use the `emcee` MCMC sampler with `astropy.modeling` astronomy-specific models implementing the likelihood function.  `astropy.uncertainty` is instead meant to provide a vehicle by which you can store uncertainties, and follow the basic error propogation rules when your science case does not require full statistical modeling.\n",
    "\n",
    "Moreover, it is a newer sub-package.  While we do not anticipate major changes, it is possible some of the interface will evolve in future versions of astropy.\n",
    "\n",
    "### *Note: This notebook is a copy of the tutorial notebook with some redundant cells omitted and with exercise solutions filled in*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary imports\n",
    "\n",
    "We start by importing some general packages we will need below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from astropy.visualization import quantity_support\n",
    "quantity_support()\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy import units as u\n",
    "\n",
    "from astropy import uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recording uncertainties of data\n",
    "\n",
    "The first use case for `uncertainty` is simply storing uncertainties *with* a quantity of interest. Lets start with a concrete and fairly typical use case: magnitudes of some set of objects drawn from an existing paper. As an example, consider the two galaxies highlighted in this paper: https://doi.org/10.1088/2041-8205/798/1/L21, which have apparent $r$-band magnitudes given with standard symmetric error bars. The typical assumption is that these values are to be thought of as having Gaussian/Normal uncertainties with the uncertainty being the $\\sigma$ of the Gaussian. Lets start by trying to represent just one of these (Pisces A) using `uncertainty`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piscA_mr = uncertainty.normal(17.35*u.mag, std=0.05*u.mag, n_samples=10000)\n",
    "piscA_mr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(piscA_mr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is immediately apparent something has happened here beyond just recording the value and its $\\sigma$.  `astropy.uncertainty` uses a Monte Carlo representation of the quantities it stores.  So when we called `uncertainty.normal`, we created a normal distribution of numbers with the given parameters.  This is why `n_samples` is required: only you, the user, knows how careful you want to be in modeling the uncertainties.  The standard choice of 10000 is reasonable, as the error in your uncertainty-related parameters generally go like $\\sqrt(N_{\\rm samples})$, so with 10000 samples you can trust your estimtes *on the uncertainties* to 1%.\n",
    "\n",
    "To illustrate what has happened, lets use some of the convenience parameters `QuantityDistribution` provides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piscA_mr.pdf_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piscA_mr.pdf_std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piscA_mr.pdf_median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above compute the mean, standard deviation, and median of the *samples* from the distribution. It is apparent that this reproduces the input we gave but only to ~1%, as expected from the number of samples.  We can also access the samples directly, as needed to produce, for example, a plot of the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(piscA_mr.distribution, bins='auto', density=True, histtype='step')\n",
    "\n",
    "plt.axvline(piscA_mr.pdf_mean(), color='k')\n",
    "plt.axvline(piscA_mr.pdf_mean() + piscA_mr.pdf_std(), color='k', ls=':')\n",
    "plt.axvline(piscA_mr.pdf_mean() - piscA_mr.pdf_std(), color='k', ls=':');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can represent both galaxies from this paper the same way,  letting us do quick convenient operations over them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piscB_mr = uncertainty.normal(17.18*u.mag, std=0.07*u.mag, n_samples=10000)\n",
    "\n",
    "for distr in [piscA_mr, piscB_mr]:\n",
    "    plt.hist(distr.distribution, bins='auto', density=True, histtype='step')\n",
    "    \n",
    "    plt.axvline(distr.pdf_mean(), color='k')\n",
    "    plt.axvline(distr.pdf_mean() + distr.pdf_std(), color='k', ls=':')\n",
    "    plt.axvline(distr.pdf_mean() - distr.pdf_std(), color='k', ls=':')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This on its own is enough to ask simple statistical questions using just the samples.  For example: what is the probability that Pisces A is brighter than Pisces B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridA, gridB = np.meshgrid(piscA_mr.distribution, piscB_mr.distribution)\n",
    "np.sum(gridA < gridB) / gridA.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your computer's speed, you may have seen a noticeable amount of time passing for that computation. This is because the `meshgrid` function created an array element for every possible pair in the two distributions, which is $10000^2 = 10^8$, meaning it had to create two 100 million element array.  This probably used a few GB of your computer's memory, a not-insignificant fraction of what you have. This is one of the \"gotchas\" to be aware of with the monte carlo method: when doing operations that involve multiple distributions you often need to compare all combinations, which quickly becomes exponentially large and overwhelms your computer.\n",
    "\n",
    "Just in case, we delete the large variables we created above. (so that if your computer has limited memory it won't become a problem later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del gridA, gridB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reasonable compromise to get around this is to just use the fact that the two distributions are independent and compare them element-wise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(piscA_mr.distribution < piscB_mr.distribution) / piscA_mr.n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still gets a similar answer as the more complete version, although it is different at the percent level as expected for $10^4$ $n_{\\rm samples}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the probability that Pisces A and Pisces B are within .1 mags of each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridA, gridB = np.meshgrid(piscA_mr.distribution, piscB_mr.distribution)\n",
    "np.sum(np.abs(gridA - gridB) < 0.1*u.mag) / gridA.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or at lower precision but a lot faster/less memory-intensive\n",
    "np.sum(np.abs(piscA_mr.distribution - piscB_mr.distribution) < 0.1*u.mag) / piscA_mr.n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array distributions\n",
    "\n",
    "Thus far this has not done much that you couldn't do by just making the Gaussians yourself.  But one of the use-cases for uncertainties as making it easier to wrap up multiple values as arrays that each contain distributions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies_mr = uncertainty.normal([17.35, 17.18]*u.mag, std=[.05, .07]*u.mag, n_samples=10000)\n",
    "galaxies_mr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies_mr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite holding a large number of samples, this looks like a single quantity of length 2.  We can still get at the samples though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies_mr.distribution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dist in galaxies_mr.distribution:\n",
    "    plt.hist(dist, bins='auto', density=True, histtype='step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a bit of string-processing and Jupyter notebook ticks, we can also use this to produce some nicer-looking quantities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mr in galaxies_mr:\n",
    "    mean = mr.pdf_mean()\n",
    "    std = mr.pdf_std()\n",
    "    lstr = '${mean:.2f} \\pm {std:.2f}$'\n",
    "    \n",
    "    # or equivalently, a one-liner using Python f-strings:\n",
    "    lstr = f'${mr.pdf_mean():.2f} \\pm {mr.pdf_std():.2f}$'\n",
    "\n",
    "    display.display(display.Latex(lstr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Distributions as Quantities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the real power of `Distribution`s is their ability to be treated just like ordinary quantities. (To refresh yourself on `Quantities` you can have a look at the [units and quantitites notebook](../03-UnitsQuantities/Astropy_Units.ipynb).)  For example, we can represent *both* galaxies from this paper as a single `Distribution`:\n",
    "\n",
    "While the above provides some conveniences, more utility comes from treating these as quantities the way you would any other quantity.  For example, suppose we wanted to convert these magnitudes to fluxes following the standard Pogson formulation of magnitudes:\n",
    "\n",
    "$m = -2.5 \\log_{10}(f)$\n",
    "\n",
    "(Note there are some more convenient ways to handle this conversion in `astropy` - see the [docs section on this in astropy.units](https://docs.astropy.org/en/stable/units/logarithmic_units.html), but here we do it by-hand to illustrate how to use `uncertainty` in a more general way.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies_rflux = 10**(galaxies_mr/(-2.5*u.mag)) * u.ABflux\n",
    "galaxies_rflux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies_rflux.pdf_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies_rflux.pdf_std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dist in galaxies_rflux.distribution:\n",
    "    plt.hist(dist, bins='auto', density=True, histtype='step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close inspection of this distribution shows that it is no longer quite Gaussian, as there is an extended tail to higher fluxes.  This is more apparent if we artifically inflate the magnitude uncertainty by a factor of 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies_mr_inflated_uncertainty = uncertainty.normal([17.35, 17.18]*u.mag, std=[.5, .7]*u.mag, n_samples=10000)\n",
    "galaxies_rflux_inflated_uncertainty = 10**(galaxies_mr_inflated_uncertainty/(-2.5*u.mag)) * u.ABflux\n",
    "for dist in galaxies_rflux_inflated_uncertainty.distribution:\n",
    "    plt.hist(dist, bins='auto', density=True, histtype='step',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And similarly, the error bars now clearly need to be assymetric, as demonstrated by comparing the standard deviation to the 16% / 84% tails of the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies_rflux_inflated_uncertainty.pdf_std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies_rflux_inflated_uncertainty.pdf_percentiles(16) - galaxies_rflux_inflated_uncertainty.pdf_median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "galaxies_rflux_inflated_uncertainty.pdf_percentiles(84) - galaxies_rflux_inflated_uncertainty.pdf_median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies_rflux_inflated_uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in galaxies_rflux_inflated_uncertainty:\n",
    "    lower, mid, upper = f.pdf_percentiles([16, 50, 84]).value/1e-7\n",
    "    lstr = f'${mid:.2} ^ {{ +{upper-mid:.2} }} _ {{ {lower-mid:.2} }} \\\\times 10^{-7}$'\n",
    "\n",
    "    display.display(display.Latex(lstr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex manipulations with other Astropy functionality\n",
    "\n",
    "While there is plenty to be done with quantities, `uncertainty` is also useful for more complex `astropy` objects.  We will illustrate this by using the `astropy.coordinates.SkyCoord` object.  This section assumes at least some familiarity with `coordinates`, so if you are confused by some of the coordinates-related operations, you may want to look at the [coordinates notebook](../04-Coordinates/astropy_coordinates.ipynb).\n",
    "\n",
    "We need to import functionality from the other parts of astropy we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import SkyCoord, EarthLocation\n",
    "from astropy.time import Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's assume you are looking at an image from an imager on a telescope at the Cerro Tololo Inter-American Observatory (CTIO).  In that image, you have a star you are interested in for some reason.  You have measured the centroid of the star in alt/az coordinates (as observed from your site at a particular time), but the conditions were not stellar (get it?), and the seeing was significantly worse than an arcsec. So you conclude your uncertainty is about an arcsec. We can encode that by creating a relevant `SkyCoord`, but providing the star as a distribution instead of a raw quantity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt = uncertainty.normal(50*u.deg, std=1*u.arcsec, n_samples=10000)\n",
    "az = uncertainty.normal(128*u.deg, std=1*u.arcsec, n_samples=10000)\n",
    "alt, az"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note `uncertainty` was perfectly happy to accept different units for the value and its `std`, and took care of the conversion for you.\n",
    "\n",
    "We can visualize this uncertainty on-sky by just plotting the distribution and letting the density of points indicate to us the probability distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(aspect='equal')\n",
    "plt.scatter(az.distribution, alt.distribution.to(u.deg).value, s=1, alpha=.25)\n",
    "\n",
    "plt.xlabel('Azimuth [deg]')\n",
    "plt.ylabel('Altitude [deg]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine you want to compare this to some stars in a catalog to match them to your observation.  But this catalog is in equatorial coordinates (ICRS RA & Dec). We need to convert our observations to that sysyem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctio = EarthLocation.of_site('CTIO')\n",
    "# if the above line fails because of internet issues, uncomment the following line and comment the above - it should give the same answer.\n",
    "#ctio = EarthLocation.from_geodetic(lon=-70.815*u.deg, lat=-30.16527777777777*u.deg, height=2215*u.m)\n",
    "\n",
    "obstime = Time('2023-12-21T03:00:00', scale='utc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = SkyCoord(ra=az, dec=alt, location=ctio, obstime=obstime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i.fk5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i.altaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altaz = SkyCoord(alt=alt, az=az, frame='altaz', location=ctio, obstime=obstime)\n",
    "icrs = altaz.transform_to('icrs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do'h!  Doesn't work.  Re-try with galactic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = uncertainty.normal(50*u.deg, std=1*u.arcsec, n_samples=10000)\n",
    "l = uncertainty.normal(128*u.deg, std=1*u.arcsec, n_samples=10000)\n",
    "gal = SkyCoord(l=l, b=b, frame='galactic')\n",
    "\n",
    "plt.subplot(aspect='equal')\n",
    "plt.scatter(gal.icrs.ra.distribution, gal.icrs.dec.distribution.to(u.deg).value, s=1, alpha=.25)\n",
    "\n",
    "plt.xlabel('Azimuth [deg]')\n",
    "plt.ylabel('Altitude [deg]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok that will work, carry on for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beware the ghost of covariances past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feh = uncertainty.normal(-1.5, std=0.1, n_samples=10000)\n",
    "afe = uncertainty.normal(0.5, std=0.1, n_samples=10000)\n",
    "\n",
    "plt.scatter(Feh.distribution, afe.distribution, s=1, alpha=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_abund = uncertainty.normal(-1.5, std=0.1, n_samples=10000)\n",
    "al_abund = uncertainty.normal(-1.0, std=0.1, n_samples=10000)\n",
    "\n",
    "afe = al_abund - fe_abund\n",
    "\n",
    "plt.scatter(fe_abund.distribution, afe.distribution, s=1, alpha=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Description of exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = 'correct'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "This tutorial covers a lot of material, but `astropy.uncertainty` has even more functionality that we were unable to cover in this workshop. For documentation on other features of `astropy.uncertainty`, check out [the astropy.uncertainty section of the Astropy documentation](http://astropy.readthedocs.org/en/stable/uncertainty/index.html)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "astropy-tutorials": {
   "author": "Erik Tollerud <erik.tollerud@gmail.com>",
   "date": "July 2015",
   "description": "Demonstrates use of astropy.coordinates for common tasks. Includes matching catalogs against each other, basic observing planning tasks, and basic usage of coordinates.",
   "link_name": "Using astropy.coordinates to Match Catalogs and Plan Observations",
   "name": "",
   "published": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
